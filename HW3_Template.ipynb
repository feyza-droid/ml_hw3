{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn import decomposition\n",
    "\n",
    "#suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLG527E - Machine Learning, Homework 3\n",
    "\n",
    "In this homework, you are supposed to implement following parts:\n",
    "     \n",
    "- **Part 1: solve an SVM optimization problem by hand (50 points)**\n",
    "\n",
    "- **Part 2: Federate a logistic regression classifier (40 points)**\n",
    "\n",
    "- **Part 3: practice feature selection (10 points)**\n",
    "     - Refer to Machine Learning Blinks 10 and 11 for this part:\n",
    "     - ML Blinks 10: https://www.youtube.com/watch?v=laeth5oT9YM&list=PLug43ldmRSo1LDlvQOPzgoJ6wKnfmzimQ\n",
    "     - ML Blinks 11: https://www.youtube.com/watch?v=mRmVKNklE9I&list=PLug43ldmRSo1LDlvQOPzgoJ6wKnfmzimQ \n",
    " \n",
    " \n",
    " \n",
    " ### Important Notes:\n",
    "   - Please complete this template and include any other necessary materials (screenshots of your handwritten solutions etc.) into the HW3 folder. Then zip it again and submit to Ninova.\n",
    "   - At Part 1, you can upload the screenshots of your handwritten solutions to the Notebook. But please be sure that your solutions are neat and can be read properly.\n",
    "   - For the Part 2, you will Federate a classifier across 20 datasets. Check the paper in Part 2.2 for more details. \n",
    "   - For the Part 3, you should implement feature selection from scratch however can use scikit-learn built-in functions for training the SVM.\n",
    "   - You can ask your questions via kamard@itu.edu.tr \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Solving SVM optimization by hand (50 points)\n",
    "\n",
    "You can insert the screenshots of your handwritten solution on Jupyter Notebook. For an example, check the cells including images. Do not forget to include your solution image file into the submitted .zip file.\n",
    "\n",
    "Some reminders for the question:\n",
    "\n",
    " - Lagrangian to optimize: $\\mathcal{L}_{primal} = \\sum_{i=1}^{n} a_{i} - \\frac{1}{2} [\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x^{j}] $ \n",
    "\n",
    "\n",
    "- Constraint: $\\sum_{i=1}^{n} \\alpha_{i} y_{i} = 0$\n",
    "\n",
    "\n",
    "- Optimal parameter: $w^{*} = \\sum_{i=1}^{n} \\alpha_{i} y_{i} x^{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 (25 points)\n",
    "\n",
    "<p style=\"float: left;\"><img src=\"images/Part1-1.png\" width = \"200\"></p>\n",
    "        \n",
    "        Given the two following training samples (n=2), provide below a step-by-step solution\n",
    "        to estimate the optimal parameters (w and b) of the hyperplane separating the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 Solution\n",
    "\n",
    "*Double click here to insert your solution.*\n",
    "<img src=\"\" width = \"200\">\n",
    "\n",
    "<comment> check with builtin function, draw the classifier, add the picture </comment>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn import svm\\n\\nX = [[2, 3], [2, -2]]\\ny = [1, -1]\\nclf = svm.SVC(kernel=\"linear\")\\nclf.fit(X, y)\\n\\nprint(clf.support_vectors_)\\nprint(clf.support_)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: delete this part\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "X = [[2, 3], [2, -2]]\n",
    "y = [1, -1]\n",
    "clf = svm.SVC(kernel=\"linear\")\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.support_vectors_)\n",
    "print(clf.support_)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 (15 points) \n",
    "If we add a third training point $x_3 = \\left [\\begin{matrix} -4 \\\\ 4 \\end{matrix}\\right] $, will that impact the hyperplane estimated using points $x_1$ and $x_2$? Answer this considering two cases where label for $x_3$ is (1) $y_3 = +1 $, (2) $y_3 = -1 $ and justify. You do not need to solve Lagrangian again, justify your answer drawing the hyperplanes and giving explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 Solution\n",
    "\n",
    "*Double click here to insert your solution.*\n",
    "<img src=\"\" width = \"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3 (10 points)\n",
    "Explain how to classify the point $x_{test} = \\left [\\begin{matrix} -4 \\\\ -3 \\end{matrix}\\right] $ using the estimated model at Part 1.1. What is the predicted label of $x_{test}$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3 Solution\n",
    "\n",
    "*Double click here to insert your solution.*\n",
    "<img src=\"\" width = \"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Federate a logistic regression classifier across 20 datasets (40 points)\n",
    "\n",
    "We set out to classify two digits from the MNIST dataset: digit 0 to which we assign the label 0 and digit 1 to which we assign the label 1 using the code of the logistic regression based on the softmax loss. You are allowed to use built-in functions from scikit-learn in the specified parts below, which you can install via one of the following commands (depending on which one you use):\n",
    "\n",
    "        > python3 -m pip install scikit-learn\n",
    "        > conda install -c conda-forge scikit-learn\n",
    "\n",
    "We have 20 local datasets distributed across 20 clients and 1 global test dataset left for testing. The function **parse_mnist_into_clients** in the following cell creates the datasets from the shared files. You do NOT need to change the function. You can simply run the following cell and it will load the datasets for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def parse_mnist_into_clients():\n",
    "    '''\n",
    "    Parses the MNIST data files into 20 local datasets for clients and 1 global test dataset. \n",
    "    Do NOT change this function!\n",
    "    \n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    local_datasets_data: The MNIST data of the local 20 dataset\n",
    "    local_datasets_label: Labels of the 20 local dataset. local_datasets_data[i] corresponds to local_datasets_label[i]\n",
    "    test_data : Global Test Data\n",
    "    test_label: Labels of the global test data\n",
    "    '''\n",
    "    data = np.load('mnist_data.npy')\n",
    "    labels = np.load('mnist_labels.npy')\n",
    "    local_datasets_data  = []\n",
    "    local_datasets_label = []\n",
    "    \n",
    "    for i in range(20):\n",
    "        local_datasets_data.append(np.copy(data[i*703:(i+1)*703]))\n",
    "        local_datasets_label.append(np.copy(labels[i*703:(i+1)*703]))\n",
    "    \n",
    "    test_data = np.copy(data[14060:])  \n",
    "    test_labels = np.copy(labels[14060:])\n",
    "    \n",
    "    return local_datasets_data, local_datasets_label, test_data, test_labels\n",
    "\n",
    "# Load the data\n",
    "client_data, client_label, test_data, test_label = parse_mnist_into_clients()\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example code of the logistic classifier using softmax loss is included below. Use this code to federate the learning of the 20 clients and test their local models on the left-out test dataset. Note that the code below is a very simple logistic regression code and the hyper parameters is not optimized. Feel free to modify it depending on what you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/ (1 + np.exp(-x))\n",
    "    \n",
    "def softmax_grad(X,y):\n",
    "    # Initializations\n",
    "    w = np.random.randn(X.shape[1],1);        # random initial point # 784\n",
    "    alpha = 10**-2\n",
    "    max_its = 10 #2000\n",
    "    \n",
    "    for k in range(max_its):\n",
    "        grad = calculate_grad(X,y,w) # gradient of the loss function wrt to w\n",
    "        grad = np.mean(grad, axis=1, keepdims=True) # stochastic gradient descent\n",
    "        w = w - alpha*grad # update rule of gradient descent\n",
    "        \n",
    "        \"\"\"\n",
    "        s = sigmoid(-y * (X.T @ w)) # sigmoid(-y * (X.T @ w))\n",
    "        grad_w = -X @ (s * y) # -X @ (s * y)\n",
    "        #grad_w = np.sum((s*y) * X.T, 0, keepdims=True)\n",
    "        \n",
    "        print(f\"grad_w {grad_w.shape}\")\n",
    "        print(f\"k {k} w {w.shape}\")\n",
    "        #grad_w = np.sum(grad_w)\n",
    "        #print(f\"grad_w {grad_w.shape}\")\n",
    "        w = w - alpha * grad_w\n",
    "        \"\"\"\n",
    "        \n",
    "    return w\n",
    "\n",
    "def calculate_grad(X,y,w): # gradient of the loss function least square cost by using sigmoid(xw + b)\n",
    "    pred = sigmoid(np.dot(X, w))\n",
    "    error = pred - y\n",
    "    temp = np.multiply(error, pred)\n",
    "    temp = np.multiply(temp, (1 - pred))\n",
    "    temp = np.dot(X.T, temp)\n",
    "    grad = 2 * temp\n",
    "    return grad\n",
    "\n",
    "def client_update(k, w):\n",
    "    E = 10\n",
    "    alpha = alpha = 10**-2\n",
    "    \n",
    "    for epoch in range(E):\n",
    "        grad = calculate_grad(X,y,w) # gradient of the loss function wrt to w\n",
    "        grad = np.mean(grad, axis=1, keepdims=True) # stochastic gradient descent\n",
    "        w = w - alpha*grad # update rule of gradient descent\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1 (10 points)\n",
    "\n",
    "Use built-in PCA from scikit-learn library to reduce the dimensionality of the training and testing samples to only two dimension (PC1 and PC2). PS: Watch <a href='https://www.youtube.com/watch?v=mRmVKNklE9I&list=PLug43ldmRSo1LDlvQOPzgoJ6wKnfmzimQ&index=45'>lecture 11</a> (beware of the eavesdropping phenomenon!). Note also that PCA should be applied to each client independently. In federated learning, clients never share data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(data):\n",
    "    pca = decomposition.PCA(n_components=2)\n",
    "    pca.fit(data)\n",
    "    reduced_data = pca.transform(data)\n",
    "    return reduced_data\n",
    "\n",
    "# for training\n",
    "client_data_reduced = []\n",
    "for X in client_data:\n",
    "    X_reduced = apply_pca(X)\n",
    "    client_data_reduced.append(X_reduced)\n",
    "    \n",
    "# for testing #TODO: should be the testing data independent of training\n",
    "test_data_reduced = apply_pca(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2 (20 points)\n",
    "\n",
    "From now on we will use the reduced 2-dimensional training and test datasets. Train a logistic classifier on the reduced 2-dimensional dataset for each client, independently, send the model weights to the server, aggregate and broadcast back to all clients.\n",
    "\n",
    "Basically, implement the **FederatedAveraging** algorithm in **(https://arxiv.org/pdf/1602.05629.pdf)** for B = infinity (i.e., each client will be trained on all its local training samples). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.15058697]\n",
      " [-0.15217407]]\n"
     ]
    }
   ],
   "source": [
    "client_weights = np.zeros((len(client_data_reduced), client_data_reduced[0].shape[1], 1)) # 20, 2, 1\n",
    "i = 0\n",
    "for X, y in zip(client_data_reduced, client_label):\n",
    "    w = softmax_grad(X, y)\n",
    "    client_weights[i] = w\n",
    "    i += 1\n",
    "\n",
    "# send the model weights to the server, aggregate\n",
    "server_mean_weights = np.mean(client_weights, axis=0)\n",
    "print(server_mean_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3 (10 points)\n",
    "\n",
    "Test the global (aggregated model) on the left-out test dataset. Plot the test classification accuracy on the global dataset against the number of communication rounds for E = 1, 5 and 10. You might need to use a small number of communication rounds if the convergence is fast. Note that B is always set to infinity. Check out Figure 2 in the paper for more details. What do you notice? Discuss your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feature selection using Variance Threshold (10 points)\n",
    "\n",
    "In this part, we will use scikit-learn library for training SVM. To remind you, you can install the scikit-learn using following commands:\n",
    "\n",
    "        > python3 -m pip install scikit-learn\n",
    "        > conda install -c conda-forge scikit-learn\n",
    "        \n",
    "There are lots of machine learning techniques that are available in scikit-learn library. In this problem we will use the **SVM** classifier with linear kernel and implement Variance Threshold feature selection method from scratch.\n",
    "   \n",
    "\n",
    "You can check the documentations on the internet to learn how to use SVM classifier and which parameters to use. Necessary functions are imported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6e2e3299191d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# This file is associated with the book\n",
    "# \"Machine Learning Refined\", Cambridge University Press, 2016.\n",
    "# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.datasets as ds\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement feature selection on handwritten digits dataset. In the following cell we load and examine dataset. As you can see the samples are 8 by 8 images where in each row of X, the images are kept as flatten vectors. Each feature represents a pixel (i.e 0th feature is (0, 0) pixel and 8th feature is (1, 0) pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_data = ds.load_digits()\n",
    "print(\"Number of samples: \", digit_data.data.shape[0])\n",
    "print(\"Number of attributes: \", digit_data.data.shape[1])\n",
    "print(\"Classes: \", digit_data.target_names)\n",
    "\n",
    "c = digit_data.images[0]\n",
    "for i in range(1, 10):\n",
    "    c = np.concatenate((c, digit_data.images[i]), 1)\n",
    "\n",
    "plt.gray() \n",
    "plt.matshow(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the data into training and testing sets in order to train the models on training set and test them on unseen test set. Do not change the random state so we will get the same train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = digit_data.data, digit_data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = True, random_state=20)\n",
    "\n",
    "print(\"Number of training samples: \", X_train.shape[0])\n",
    "print(\"Number of testing samples: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the following cell, you should complete the necessary train and test functions for SVM. Fill the specified parts only. Calculate the accuracy score as:\n",
    "\n",
    "<br>\n",
    "<center> $ Accuracy = \\frac{\\#correct \\ predictions}{\\#total \\ samples} $ </center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y):\n",
    "    \n",
    "    classifier = ...  # define the classifier\n",
    "    classifier = ...  # fit the classifier on training set\n",
    "    \n",
    "    preds = ...  # predict the labels for training set\n",
    "    \n",
    "    train_accuracy = ...  # calculate the accuracy (do not use built-in function)\n",
    "    \n",
    "    return classifier, train_accuracy\n",
    "\n",
    "def test(classifier, X, y):\n",
    "    \n",
    "    test_accuracy = ...  # calculate the accuracy on test set\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next, use these function to train and test an SVM without feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM classifier without feature selection\n",
    "svm, train_acc = ... # call train function with necessary parameters\n",
    "test_acc = ...  # call test function with necessary parameters\n",
    "\n",
    "print(\"Train acc without feature selection: \", train_acc)\n",
    "print(\"Test acc without feature selection: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, implement Variance Threshold feature selection method on the dataset. The Variance Threshold basicly follows these steps:\n",
    "  - Calculate the variances of each feature in the dataset. (Hint: You can use numpy for this)\n",
    "  - Eliminate the features that has variance less then the given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: YOUR CODE GOES HERE##\n",
    "\n",
    "def variance_threshold(X, threshold=0):\n",
    "    \n",
    "    selected = None # keep the indexes of selected feature\n",
    "    eliminated = None # keep the indexes of eliminated features\n",
    "    X_reduced = None # X with reduced feature set\n",
    "    \n",
    "    #calculate variances of each feature and eliminate features with variance lower than threshold.\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Indexes of the eliminated features: \", eliminated) # print the indexes of eliminated features.\n",
    "    \n",
    "    print(\"Number of features for threshold = \", threshold, \"is :\", X_reduced.shape[1])\n",
    "    return X_reduced, selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, call the variance threshold function for **5** distinct threshold values and obtain **different set of features**. Then, train and test an SVM classifier on each set. Do not forget to apply the feature selection on the test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: YOUR CODE GOES HERE##\n",
    "\n",
    "thresholds = [...]\n",
    "\n",
    "for th in thresholds:\n",
    "    print(\"Threshold = \", th)\n",
    "    X_reduced, selected = variance_threshold(...)\n",
    "    X_reduced_test = ...\n",
    "\n",
    "    svm, train_acc = ... # call train function with necessary parameters\n",
    "    test_acc = ...  # call test function with necessary parameters\n",
    "\n",
    "    print(\"Train acc with feature selection: \", train_acc)\n",
    "    print(\"Test acc with feature selection: \", test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Question (5 points)**\n",
    "Why the model performance varies with the number of selected features? Justify your answer with a toy example (i.e., using a simulated dataset). You can also refer to the results you got in Part 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Double click here to insert your answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
